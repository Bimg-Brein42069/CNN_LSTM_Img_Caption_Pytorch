{"cells":[{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T14:44:29.454844Z","iopub.status.busy":"2023-05-13T14:44:29.453942Z","iopub.status.idle":"2023-05-13T14:44:30.594534Z","shell.execute_reply":"2023-05-13T14:44:30.593516Z","shell.execute_reply.started":"2023-05-13T14:44:29.454799Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import os  # when loading file paths\n","import pandas as pd  # for lookup in annotation file\n","import spacy  # for tokenizer\n","import torch\n","from torch.nn.utils.rnn import pad_sequence  # pad batch\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image  # Load img\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torchtext.data.metrics import bleu_score\n","from nltk.translate.bleu_score import corpus_bleu\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate.meteor_score import meteor_score\n","\n","import torch\n","import torch.nn as nn\n","import statistics\n","import torchvision.models as models\n","from torchvision.models import inception_v3, Inception_V3_Weights\n","import random\n","\n","\n","spacy_eng = spacy.load(\"en_core_web_sm\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["textDirPath = \"Flickr8K/Flickr8k_text/\" \n","pathDirImage = \"Flickr8K/Flicker8k_Images/\"\n","\n","tokenLemmaFile = pd.read_csv(\"Flickr8K/Flickr8k_text/Flickr8k.lemma.token.txt\",sep = \"\\t\", on_bad_lines='skip' , header = None )\n","token = pd.read_csv(\"Flickr8K/Flickr8k_text/Flickr8k.token.txt\",sep = \"\\t\",on_bad_lines='skip', header = None)\n","validationData = pd.read_csv(\"Flickr8K/Flickr8k_text/Flickr_8k.valImages.txt\", on_bad_lines = 'skip', header = None)\n","trainingData = pd.read_csv(\"Flickr8K/Flickr8k_text/Flickr_8k.trainImages.txt\",on_bad_lines='skip', header = None)\n","testingData = pd.read_csv(\"Flickr8K/Flickr8k_text/Flickr_8k.testImages.txt\",on_bad_lines='skip', header = None)\n","\n","# renaming columns with Image and caption\n","tokenLemmaFile = tokenLemmaFile.rename(columns = {0 : \"Image\" , 1 : \"caption\"})\n","\n","# splitting using '#' token \n","tokenLemmaFile[\"Image\"] =[i.split('#')[0] for i in tokenLemmaFile[\"Image\"].values ]\n","\n","\n","# drop the index column \n","\n","# validation file \n","captionVal = tokenLemmaFile.where(tokenLemmaFile[\"Image\"].isin(validationData[0].values) == True).dropna().reset_index().drop('index' , axis = 1)\n","\n","# training file \n","captionTrain = tokenLemmaFile.where(tokenLemmaFile[\"Image\"].isin(trainingData[0].values) == True).dropna().reset_index().drop('index' , axis = 1)\n","\n","# testing file\n","captionTest = tokenLemmaFile.where(tokenLemmaFile[\"Image\"].isin(testingData[0].values) == True).dropna().reset_index().drop('index' , axis = 1)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                           Image  \\\n","0      1305564994_00513f9a5b.jpg   \n","1      1305564994_00513f9a5b.jpg   \n","2      1305564994_00513f9a5b.jpg   \n","3      1305564994_00513f9a5b.jpg   \n","4      1305564994_00513f9a5b.jpg   \n","...                          ...   \n","29995   985067019_705fe4a4cc.jpg   \n","29996   985067019_705fe4a4cc.jpg   \n","29997   985067019_705fe4a4cc.jpg   \n","29998   985067019_705fe4a4cc.jpg   \n","29999   985067019_705fe4a4cc.jpg   \n","\n","                                                 caption  \n","0      A man in street racer armor be examine the tir...  \n","1             Two racer drive a white bike down a road .  \n","2      Two motorist be ride along on their vehicle th...  \n","3      Two person be in a small race car drive by a g...  \n","4           Two person in race uniform in a street car .  \n","...                                                  ...  \n","29995                A boy go down an inflatable slide .  \n","29996       A boy in red slide down an inflatable ride .  \n","29997               A boy be slide down in a red shirt .  \n","29998              A child go down an inflatable slide .  \n","29999  A young boy slide down an inflatable , be look...  \n","\n","[30000 rows x 2 columns]\n"]}],"source":["print(captionTrain)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T14:44:30.596983Z","iopub.status.busy":"2023-05-13T14:44:30.596612Z","iopub.status.idle":"2023-05-13T14:44:32.658179Z","shell.execute_reply":"2023-05-13T14:44:32.657110Z","shell.execute_reply.started":"2023-05-13T14:44:30.596949Z"},"trusted":true},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self, freq_threshold):\n","        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n","        self.freq_threshold = freq_threshold\n","\n","    def __len__(self):\n","        return len(self.itos)\n","\n","    @staticmethod\n","    def tokenizer_eng(text):\n","        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n","\n","    def build_vocabulary(self, sentence_list):\n","        frequencies = {}\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            for word in self.tokenizer_eng(sentence):\n","                if word not in frequencies:\n","                    frequencies[word] = 1\n","\n","                else:\n","                    frequencies[word] += 1\n","\n","                if frequencies[word] == self.freq_threshold:\n","                    self.stoi[word] = idx\n","                    self.itos[idx] = word\n","                    idx += 1\n","\n","    def numericalize(self, text):\n","        tokenized_text = self.tokenizer_eng(text)\n","\n","        return [\n","            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n","            for token in tokenized_text\n","        ]\n","\n","\n","class FlickrDataset(Dataset):\n","    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n","        self.root_dir = root_dir\n","        #self.df = pd.read_csv(captions_file)\n","        self.df = captions_file\n","        self.transform = transform\n","\n","        # Get img, caption columns\n","        self.imgs = self.df[\"Image\"]\n","        self.captions = self.df[\"caption\"]\n","\n","        # Initialize vocabulary and build vocab\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocabulary(self.captions.tolist())\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        caption = self.captions[index]\n","        img_id = self.imgs[index]\n","        filename = self.root_dir + \"/\" + img_id\n","        img = Image.open(filename)\n","        #img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n","        numericalized_caption += self.vocab.numericalize(caption)\n","        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n","\n","        return img, torch.tensor(numericalized_caption)\n","\n","\n","class MyCollate:\n","    def __init__(self, pad_idx):\n","        self.pad_idx = pad_idx\n","\n","    def __call__(self, batch):\n","        imgs = [item[0].unsqueeze(0) for item in batch]\n","        imgs = torch.cat(imgs, dim=0)\n","        targets = [item[1] for item in batch]\n","        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n","\n","        return imgs, targets\n","\n","\n","def get_loader(\n","    root_folder,\n","    annotation_file,\n","    transform,\n","    batch_size=32,\n","    num_workers=2,\n","    shuffle=True,\n","    pin_memory=True,\n","):\n","    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n","\n","    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","    #pad_idx = 0\n","\n","    loader = DataLoader(\n","        dataset=dataset,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        shuffle=shuffle,\n","        pin_memory=pin_memory,\n","        collate_fn=MyCollate(pad_idx=pad_idx),\n","    )\n","\n","    return loader, dataset\n","\n","\n","\n","transform = transforms.Compose(\n","        [transforms.Resize((224, 224)), transforms.ToTensor(),]\n",")\n","\n","#loader, dataset = get_loader(\n","#    \"Flickr8k/Images\", \"flickr8k/captions.txt\", transform=transform\n","#)\n","\n","# for idx, (imgs, captions) in enumerate(loader):\n","#     print(imgs.shape)\n","#     print(captions.shape)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T14:44:32.660269Z","iopub.status.busy":"2023-05-13T14:44:32.659890Z","iopub.status.idle":"2023-05-13T14:44:32.673217Z","shell.execute_reply":"2023-05-13T14:44:32.672222Z","shell.execute_reply.started":"2023-05-13T14:44:32.660236Z"},"trusted":true},"outputs":[],"source":["def print_examples(model, device, dataset):\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize((299, 299)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","        ]\n","    )\n","\n","    model.eval()\n","    test_img1 = transform(Image.open(\"test_examples/dog.jpg\").convert(\"RGB\")).unsqueeze(\n","        0\n","    )\n","    print(\"Example 1 CORRECT: Dog on a beach by the ocean\")\n","    print(\n","        \"Example 1 OUTPUT: \"\n","        + \" \".join(model.caption_image(test_img1.to(device), dataset.vocab))\n","    )\n","    test_img2 = transform(\n","        Image.open(\"test_examples/child.jpg\").convert(\"RGB\")\n","    ).unsqueeze(0)\n","    print(\"Example 2 CORRECT: Child holding red frisbee outdoors\")\n","    print(\n","        \"Example 2 OUTPUT: \"\n","        + \" \".join(model.caption_image(test_img2.to(device), dataset.vocab))\n","    )\n","    test_img3 = transform(Image.open(\"test_examples/bus.png\").convert(\"RGB\")).unsqueeze(\n","        0\n","    )\n","    print(\"Example 3 CORRECT: Bus driving by parked cars\")\n","    print(\n","        \"Example 3 OUTPUT: \"\n","        + \" \".join(model.caption_image(test_img3.to(device), dataset.vocab))\n","    )\n","    test_img4 = transform(\n","        Image.open(\"test_examples/boat.png\").convert(\"RGB\")\n","    ).unsqueeze(0)\n","    print(\"Example 4 CORRECT: A small boat in the ocean\")\n","    print(\n","        \"Example 4 OUTPUT: \"\n","        + \" \".join(model.caption_image(test_img4.to(device), dataset.vocab))\n","    )\n","    test_img5 = transform(\n","        Image.open(\"test_examples/horse.png\").convert(\"RGB\")\n","    ).unsqueeze(0)\n","    print(\"Example 5 CORRECT: A cowboy riding a horse in the desert\")\n","    print(\n","        \"Example 5 OUTPUT: \"\n","        + \" \".join(model.caption_image(test_img5.to(device), dataset.vocab))\n","    )\n","    model.train()\n","\n","\n","def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","    step = checkpoint[\"step\"]\n","    return step"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Attention(nn.Module):\n","\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n","        super(Attention, self).__init__()\n","        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n","        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n","        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n","\n","    def forward(self, encoder_out, decoder_hidden):\n","        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n","        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n","        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n","        alpha = self.softmax(att)  # (batch_size, num_pixels)\n","        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n","\n","        return attention_weighted_encoding, alpha\n","\n","\n","class DecoderWithAttention(nn.Module):\n","\n","    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5,teacher_forcing_ratio = 1):\n","        super(DecoderWithAttention, self).__init__()\n","\n","        self.encoder_dim = encoder_dim\n","        self.attention_dim = attention_dim\n","        self.embed_dim = embed_dim\n","        self.decoder_dim = decoder_dim\n","        self.vocab_size = vocab_size\n","        self.dropout = dropout\n","\n","        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n","\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n","        for p in self.embedding.parameters():\n","            p.requires_grad = True\n","        \n","        self.dropout = nn.Dropout(p=self.dropout)\n","        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n","        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n","        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n","        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n","        self.sigmoid = nn.Sigmoid()\n","        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n","        self.init_weights()  # initialize some layers with the uniform distribution\n","        self.teacher_forcing_ratio = teacher_forcing_ratio\n","\n","    def init_weights(self):\n","        self.embedding.weight.data.uniform_(-0.1, 0.1)\n","        self.fc.bias.data.fill_(0)\n","        self.fc.weight.data.uniform_(-0.1, 0.1)\n","            \n","\n","    def forward(self, encoder_out, encoded_captions, caption_lengths):\n","        batch_size = encoder_out.size(0)\n","        encoder_dim = encoder_out.size(-1)\n","        vocab_size = self.vocab_size\n","\n","        # Flatten image\n","        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n","        num_pixels = encoder_out.size(1)\n","\n","        # Sort input data by decreasing lengths; why? apparent below\n","        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n","        encoder_out = encoder_out[sort_ind]\n","        encoded_captions = encoded_captions[sort_ind]\n","\n","        # Embedding\n","        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n","\n","        # Initialize LSTM state\n","        mean_encoder_out = encoder_out.mean(dim=1)\n","        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n","        c = self.init_c(mean_encoder_out)\n","\n","        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n","        # So, decoding lengths are actual lengths - 1\n","        decode_lengths = (caption_lengths - 1).tolist()\n","\n","        # Create tensors to hold word predicion scores and alphas\n","        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n","        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n","\n","        # At each time-step, decode by\n","        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n","        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n","        preds = embeddings[:,0,:]\n","        for t in range(max(decode_lengths)):\n","            batch_size_t = sum([l > t for l in decode_lengths])\n","            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n","                                                                h[:batch_size_t])\n","            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n","            attention_weighted_encoding = gate * attention_weighted_encoding\n","            teacher_force = random.random() < self.teacher_forcing_ratio\n","            top1 = preds.argmax(1)\n","            embeds_temp = self.embedding(top1)\n","            next_input = embeddings[:batch_size_t, t, :] if teacher_force else embeds_temp[:batch_size_t,:]\n","            h, c = self.decode_step(\n","                torch.cat([next_input, attention_weighted_encoding], dim=1),\n","                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n","            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n","            predictions[:batch_size_t, t, :] = preds\n","            alphas[:batch_size_t, t, :] = alpha\n","            \n","        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T14:44:32.676371Z","iopub.status.busy":"2023-05-13T14:44:32.675816Z","iopub.status.idle":"2023-05-13T14:44:32.695310Z","shell.execute_reply":"2023-05-13T14:44:32.694449Z","shell.execute_reply.started":"2023-05-13T14:44:32.676339Z"},"trusted":true},"outputs":[],"source":["\n","\n","class EncoderCNN(nn.Module):\n","    def __init__(self, embed_size, train_CNN=False):\n","        super(EncoderCNN, self).__init__()\n","        self.train_CNN = train_CNN\n","#         self.inception = models.inception_v3(pretrained=True, aux_logits=True)\n","        self.inception = inception_v3(Inception_V3_Weights.DEFAULT)\n","        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n","        self.relu = nn.ReLU()\n","        self.times = []\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, images):\n","        self.inception.aux_logits = False\n","        features = self.inception(images)\n","        self.inception.aux_logits = True\n","        return self.dropout(self.relu(features))\n","\n","\n","class DecoderRNN(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","        super(DecoderRNN, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, features, captions):\n","        embeddings = self.dropout(self.embed(captions))\n","        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n","        hiddens, _ = self.lstm(embeddings)\n","        outputs = self.linear(hiddens)\n","        return outputs\n","\n","\n","class CNNtoRNN(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers,attention_size, use_attn=0):\n","        super(CNNtoRNN, self).__init__()\n","        self.encoderCNN = EncoderCNN(embed_size)\n","        if not use_attn:\n","            self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n","        else:\n","            self.decoderRNN = DecoderWithAttention(embed_size, hidden_size, vocab_size, num_layers)\n","\n","    def forward(self, images, captions):\n","        features = self.encoderCNN(images)\n","        outputs = self.decoderRNN(features, captions)\n","        return outputs\n","\n","    def caption_image(self, image, vocabulary, max_length=50):\n","        result_caption = []\n","\n","        with torch.no_grad():\n","            x = self.encoderCNN(image).unsqueeze(0)\n","            states = None\n","\n","            for _ in range(max_length):\n","                hiddens, states = self.decoderRNN.lstm(x, states)\n","                output = self.decoderRNN.linear(hiddens.squeeze(0))\n","                predicted = output.argmax(1)\n","                lmao=predicted[0].item()\n","                result_caption.append(lmao)\n","                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n","\n","                if vocabulary.itos[lmao] == \"<EOS>\":\n","                    break\n","\n","        return [vocabulary.itos[idx] for idx in result_caption]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T14:44:32.698316Z","iopub.status.busy":"2023-05-13T14:44:32.697811Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/dspani/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["=> Saving checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["                                                 \r"]}],"source":["import torch\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.tensorboard import SummaryWriter\n","\n","def train():\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize((356, 356)),\n","            transforms.RandomCrop((299, 299)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","        ]\n","    )\n","\n","\n","    train_loader, dataset = get_loader(root_folder=pathDirImage,annotation_file=captionTrain,transform=transform,num_workers=2)\n","\n","    torch.backends.cudnn.benchmark = True\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    load_model = False\n","    save_model = True\n","    train_CNN = False\n","\n","    # Hyperparameters\n","    embed_size = 256\n","    hidden_size = 256\n","    vocab_size = len(dataset.vocab)\n","    num_layers = 1\n","    use_attentn = 0\n","    attention_dim = 256\n","    learning_rate = 3e-4\n","    num_epochs = 10\n","\n","    # for tensorboard\n","    writer = SummaryWriter(\"runs/flickr\")\n","    step = 0\n","\n","    # initialize model, loss etc\n","    model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers,attention_dim,0).to(device)\n","    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Only finetune the CNN\n","    for name, param in model.encoderCNN.inception.named_parameters():\n","        if \"fc.weight\" in name or \"fc.bias\" in name:\n","            param.requires_grad = True\n","        else:\n","            param.requires_grad = train_CNN\n","\n","    if load_model:\n","        step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n","\n","    model.train()\n","\n","    for epoch in range(num_epochs):\n","        # Uncomment the line below to see a couple of test cases\n","        # print_examples(model, device, dataset)\n","\n","        if save_model:\n","            checkpoint = {\n","                \"state_dict\": model.state_dict(),\n","                \"optimizer\": optimizer.state_dict(),\n","                \"step\": step,\n","            }\n","            save_checkpoint(checkpoint)\n","\n","        for idx, (imgs, captions) in tqdm(enumerate(train_loader), total=len(train_loader), leave=False):\n","            imgs = imgs.to(device)\n","            captions = captions.to(device)\n","\n","            outputs = model(imgs, captions[:-1])\n","            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n","\n","            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n","            step += 1\n","\n","            optimizer.zero_grad()\n","            loss.backward(loss)\n","            optimizer.step()\n","\n","    return model\n","\n","max_split_size_mb=512\n","model = train()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["'transform = transforms.Compose(\\n    [transforms.Resize((224, 224)), transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\\n)\\nval_loader, dataset = get_loader(\\n        pathDirImage,\\n        captionVal,\\n        transform=transform,\\n        num_workers=2,\\n    )\\n\\nlossesVal = [] \\nfor indices, (imgs, captions) in tqdm(\\n            enumerate(val_loader), total=len(val_loader), leave=False ):\\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n            imgs = imgs.to(device)\\n            captions = captions.to(device)\\n\\n            Output = model(imgs, captions[:-1])\\n            model.caption_image(imgs) \\n            technique = nn.CrossEntropyLoss(ignore_index=Dataset.vocab.stoi[\"<PAD>\"])\\n            loss = technique(\\n                Output.reshape(-1, Output.shape[2]), captions.reshape(-1)\\n            )\\n            lossesVal.append(loss)'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["'''transform = transforms.Compose(\n","    [transforms.Resize((224, 224)), transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",")\n","val_loader, dataset = get_loader(\n","        pathDirImage,\n","        captionVal,\n","        transform=transform,\n","        num_workers=2,\n","    )\n","\n","lossesVal = [] \n","for indices, (imgs, captions) in tqdm(\n","            enumerate(val_loader), total=len(val_loader), leave=False ):\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            imgs = imgs.to(device)\n","            captions = captions.to(device)\n","\n","            Output = model(imgs, captions[:-1])\n","            model.caption_image(imgs) \n","            technique = nn.CrossEntropyLoss(ignore_index=Dataset.vocab.stoi[\"<PAD>\"])\n","            loss = technique(\n","                Output.reshape(-1, Output.shape[2]), captions.reshape(-1)\n","            )\n","            lossesVal.append(loss)'''\n","            #if indices % 500 : \n","            #  print(loss)\n","\n","#losses = []\n","#for i in lossesVal :\n","#  losses.append(i.cpu().detach().numpy())"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["#plt.plot(pd.Series(losses).rolling(100).mean().dropna())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transform = transforms.Compose(\n","        [\n","            transforms.Resize((356, 356)),\n","            transforms.RandomCrop((299, 299)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","        ]\n","    )\n","\n","\n","test_loader, dataset = get_loader(\n","        pathDirImage,\n","        captionTest,\n","        transform=transform,\n","        num_workers=2,\n","    )\n","\n","rl_caps1 =[]\n","rl_caps =[]\n","for indices, (imgs, captions) in enumerate(test_loader):\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            imgs = imgs.to(device)\n","            captions = captions.to(device)\n","            Output = model(imgs, captions[:-1])\n","\n","\n","            for cap in captions.tolist():\n","                cap = [dataset.vocab.itos[idx] for idx in cap]\n","                rl_caps.append(cap)\n","\n","\n","\n","            caption = model.caption_image(imgs.to(device), dataset.vocab)\n","            rl_caps1.append(caption)\n","            #bleu_score(caption.reshape(-1, Output.shape[2]), captions.reshape(-1))\n","\n","bleu4 = 0\n","cnt=0\n","for i in range(len(rl_caps1)):\n","    cnt+=1\n","    bleu4+=sentence_bleu(rl_caps[i],rl_caps1[i])\n","bleu4/=cnt\n","\n","meteor=0\n","cnt=0\n","for i in range(len(rl_caps1)):\n","    cnt+=1\n","    meteor+=meteor_score(rl_caps[i],rl_caps1[i])\n","meteor/=cnt"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5642091925687237\n"]}],"source":["print(bleu4)"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5031928922467322\n"]}],"source":["print(meteor)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"vscode":{"interpreter":{"hash":"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"}}},"nbformat":4,"nbformat_minor":4}
